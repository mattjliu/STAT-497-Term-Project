\documentclass[a4paper,12pt]{article}

% Packages
\usepackage{float}
\usepackage[utf8]{inputenc}
\usepackage{fancyhdr}
\usepackage[left = 0.6 in, right = 0.6 in, top = 1 in, bottom = 1 in, headsep = 0.5 in]{geometry}
\usepackage[normalem]{ulem}
\usepackage{enumerate}
\usepackage{pgfplots}
\usepackage{titling}
\pgfplotsset{width=8cm,compat=1.9}
\usepackage{stackengine}
\usepackage{algorithm}
\usepackage[noend]{algpseudocode}
%\pagenumbering{gobble} 
\usepackage{amsmath} % add [fleqn] before {amsmath} to left center
\usepackage{amssymb}
\usepackage[T1]{fontenc}
\usepackage{fancybox}
\usepackage{longtable}
\usepackage{tikz}


\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

\hfuzz = 100pt

% For heaps
\tikzset{
  heap/.style={
    every node/.style={circle,draw},
    level 1/.style={sibling distance=30mm},
    level 2/.style={sibling distance=15mm}
  }
}


%%% Horizontal Line Command
\makeatletter
  \newcommand*\variableheghtrulefill[1][.4\p@]{%
    \leavevmode
    \leaders \hrule \@height #1\relax \hfill
    \null
  }
\makeatother
%%%

%%% Algorithm commands
\algblock{Input}{EndInput}
\algnotext{EndInput}
\algblock{Output}{EndOutput}
\algnotext{EndOutput}
\newcommand{\Desc}[2]{\State \makebox[3em][l]{#1}#2}
%%%

% %%% Header & Footer
% \fancyhf{}
% \renewcommand{\footrulewidth}{0.1mm}
% \fancyhead[L]{Matteo Esposito}
% \fancyhead[R]{COMP 352-X}
% \fancyfoot[R]{\thepage}

% \pagestyle{fancy}
% %%%

% Code formatting
\newcommand{\code}[1]{\texttt{#1}}

\renewcommand\maketitlehooka{\null\mbox{}\vfill}
\renewcommand\maketitlehookd{\vfill\null}

\setlength{\abovedisplayskip}{2pt}
\setlength{\belowdisplayskip}{3pt}

% Custom settings
\setlength{\parskip}{0.75em}  % Paragraph spacing
\setcounter{section}{-1} % Page numbers to start on page 1
\setlength\parindent{0pt} % Remove indenting from entire file
\def\layersep{2.5cm}

\title{
{STAT 497 - Final Project} \\
{\large Concordia University} \\
}
\date{}

%--------------------------------------------------------------------------%

\begin{document}

\begin{titlingpage}
  \maketitle
  \centering
  \vfill
  {\large{Bakr Abbas, Matthew Liu, Frederic Siino}} \par
  {\large{Day Month Year}}
\end{titlingpage}

\newpage

\section{Introduction}
One of the main challenges of managing a portfolio or a fund, is the decision process of how to allocate investment resources to each assets. The goal is to maximize returns but also to diversify, putting all the fund in the most profitable asset could be beneficial in the short run, but the possibility of that one asset crashing in value is larger than the possibility of multiple assets crashing in value. So diversifying is important to hedge against that outcome, and portfolio management in the modern day involves more automation than ever. Reinforcement \& Deep learning methods are being used to optimize this decision making process. \\

Cryptocurrencies are decentralized digital assets that behave as an alternative to government issued money. They are being introduced into most investment portfolios, this paper address the challenge of managing five different cryptocurrencies with Bitcoin(BTC) being cash.Meaning that the four other currencies are quantified as the price relative to BTC.  DIfferent methods will be used to manage the weights assigned to each asset in the portfolio. The first three methods will be based on the contextual bandits model.The final method will be based on a policy gradient model. A more realistic model would also take transaction costs into account, but this will be set aside for simplicity. \\

A previous research paper (Jiang, Xu, Liang, 2017) has been published on this exact topic based on a framework consisting of Ensemble of Identical Independent Evaluators (EIIE), a Portfolio-Vector Memory (PVM), an Online Stochastic Batch Learning (OSBL) scheme and a fully exploiting strategy. The actions were determined using a policy gradient method and the tested evaluators were a Convolutional Neural Network (CNN), a Recurrent Neural Network (RNN) and a Long Short-Term Memory Network (LSTM). Experiments were run with a trading period of 30 minutes and the RL framework was observed to behave far better than other optimization frameworks in the test period of 2017 but inferior to some frameworks in the test period of 2018. \\

In this project, we opt for a much simpler approach, relying instead on an action preference function and a policy gradient when framing the task as a contextual bandits problem. We do however borrow some aspects of Jiang, Xu and Liang such as definitions for states and returns. \\

\section{Problem description}
In this project, the environment is solely dictated by the data of the historic prices of all cryptocurrencies relative to BTC. In the experiments, a trading period of $T = 120$ minutes is used, and we only consider the closing prices of each period (which are equivalent to the opening prices of the next period). In addition, the experiments rely on two key assumptions (Jiang, Xu, Liang, 2017):

\begin{itemize}
  \item Instant liquidity: The liquidity of the market allows each trade to be carried out immediately at the price listed when the order is placed.
  \item No market impact: The capital invested by the agent is so insignificant as to have no impact on the market.
\end{itemize}

This means that the sequence of states and transition probabilities are already predefined based on the data at hand. In particular, we define a price relative vector $y_t$ of the $t$th trading period like Jiang, Xu and Liang, namely:
$$y_t := v_t \oslash v_{t-1} = \bigg(1,\frac{v_{2,t}}{v_{2,t-1}}, â€¦,\frac{v_{m-1,t}}{v_{m-1,t-1}}, \frac{v_{m,t}}{v_{m,t-1}}\bigg)^T$$ 
where $v_{t,i}$ denotes the closing price of the $i$th asset in the $t$th trading period. Note here that asset 1 and thus $v_{t,1}$ is always equal to 1. This is because the relative prices $v_{t,i}$ are relative to the first asset (in our case, BTC), also known as the cash. This price relative vector can then be used to calculate the reward, defined in general as 
$$r_t = y_t \cdot w_{t-1}$$
where $w_{t-1}$ is the portfolio weight vector at time $t-1$. This reward is effectively the change in portfolio value for period $t$. The action $a_t$ at time $t$ is the portfolio weight vector $w_t$, where the $i$th element is the proportion of asset $i$ in the portfolio. The goal of the agent is then to maximize the growth rate (or cumulative return) $r_t$ of the portfolio over the experimental time frame. \\

In addition, we also define a state vector $S_T := (s_{1,T},...,s_{m,T})$ where $s_{i,T}$ denotes the state of the previous price changes of asset $i$ at time $T$. Namely,
$$s_{i,T} = \sum_{t=T-n}^{t=T} \gamma^t y_{i,t}$$ 
where $y_{i,t}$ is the $i$th element of the previously defined price relative vector $y_t$, $\gamma$ is a discount factor and $n$ is the size of the history we consider. As such the state is a vector of $n$-step backward relative price changes for each asset discounted by a rate gamma. \\

Thus, based on the environment and problem definition, we have 
$$p(s_{t+1} | s_t,a) := Pr[S_{t+1}=s_{t+1} | S_t=s_t,A_t=a] = 1 \; \forall a \in A$$
for the deterministic sequence of states
$$S_1,S_1,...,S_{T-n}$$
which follows since the sequence $y_1,y_2,...,y_T$ is determined by the data. \\

For the sake of simplicity, an additional assumption that there are no transaction fees is added. Thus the focus turns to wether or not the agent is able to learn the optimal re-weighting of the portfolio given the state at each time, i.e. the agent must identify promising assets. The question of transaction fees and practicality will be judged in a seperate analysis. \\

Lastly, since we let the agent run through all the available data when conducting an experiment, the problem becomes an episodic task where $t=0,...,T$ and $T$ is the number of trading periods in our data set. In practice however, the problem is a continuing task since the agent is tasked to a manage a portfolio forever.

\section{Analyses}
Analysis goes here

\subsection{Method 1}
Method 1 is an ad-hoc method based on the contextual bandits problem. The portfolio optimization problem is reframed as a k-armed bandits problem where we have one arm for each asset in the portfolio (including cash). The action preference method is used, where 
$$H_t(a), \; a = 1,...,k$$
are the action preferences for all k assets. However, a slight modification is done when defining actions and updating the preference functions. When computing probabilities of chosing a given action, we define $\pi_t (a)$ in the standard way, i.e.
$$\pi_t (a) := Pr[A_t=a | R_1,...,R_{t-1}] = \frac{e^{H_t(a) S_{a,t}}}{\sum_{b=1}^{k} e^{H_t(b) S_{b,t}}}$$
where the previously defined state vector $S_t$ is included due to the contextual nature of the problem. The $n$-size history of discounted price changes represented by $S_t$ will thus change the preferences of each asset based on whether or not their prices are observed to have declined or increased in the past $n$ periods (uptrends increase preference, and downtrends decrease preference). Following the standard contextual bandits framework, this leads to the action at time $t$ of
$$A_t = \argmax_a \pi_t(a)$$
In the form of a portfolio weight vector, $A_t = w_t$ is a $k$-length vector where the $A_t$th co-ordinate is 1 and all other co-ordinates are 0. This means that rewards are
$$R_{t+1} := y_{t+1} \cdot A_t = y_{t+1,A_t}$$
i.e., the relative price change of asset $A_t$. The problem then reduces to a simple contextual bandits problem where one and only one arm is pulled by the agent at each time steps. \par
However, in the ad-hoc method the actual action taken by the agent at time $t$ is the vector of preference values, i.e. 
$$A_t' = (H_t(1),H_t(2),...,H_t(k))$$ 
Since all $H_t(k)$'s sum to 1, this is a valid portfolio weight vector, and we can set $A_t' = w_t$. The rewards are then computed as 
$$R_{t+1}' :=  y_{t+1} \cdot A_t'$$
and the preference functions are updated based on $R_t'$ as follows:
$$H_{t+1}(A_t) = H_t(A_t) + \alpha (R_t' - \bar{R_t})(a - \pi_t(A_t))$$ 
$$H_{t+1}(a) = H_t(a) - \alpha (R_t' - \bar{R_t})\pi_t(a), \; a \neq A_t$$ \\
Intuitively, this method relies on the simplfying assumption that rewards produced by actions $A_t'$ is a good enough approximation of rewards produced by actions $A_t$. In other words, 
$$E[R_t'] \approx E[R_t]$$
In reality however, we can predict that this is far from the case (hence the ad-hoc nature of this method). It is safe to assume that computing the returns of a preference vector of assets is not the same as computing the total one-step return of the single most preferred asset. Nevertheless, the ad-hoc method provides a straightforward way to implement an algorithm learning to optimize a portfolio of assets since the softmax produces valid actions that represent the preference of the agent at time $t$ based on $S_t$. However, the actual rewards used to update the preference function is only a weak approximation of the actual reward $R_t$.

The function used to simulate a single episode of method 1 is \code{simulate\_contextual1} and the parameters are as follows:
\begin{itemize}
  \item \code{n\_curren}: The number of currencies in the problem including cash (5 for this project)
  \item \code{n\_steps}: The number of timesteps $T$ in the episode. To use all available data set \code{n\_steps = nrow(data)}
  \item \code{alpha}: Learning rate $\alpha$ when updating action preference function (as defined above)
  \item \code{window\_size}: Size of historic prices to consider when computing $S_t$ (\code{window\_size} corresponds to the value of $n$)
  \item \code{discount}: Discount factor of previous relative price changes when computing $S_t$ (\code{discount} corresponds to the value of $\gamma$)
\end{itemize}



\subsection{Method 2}
Analysis of method 2

\section{Conclusion}
Conclusion goes here

\section{References}
References go here

\end{document}

